The scripts were written and tested with Python 2.7.4 on Ubuntu 14.04 LTS.
Performance benchmarks run on an Asus G751JY (i7 4770 HQ/GTX 980m).


--USAGE--
For tex files in folder sample (e.g. sample/example.tex):

python getarxivdatav2.py sample (produces 'sample.txt in cwd')
python parsetex.py sample (produces & populates folder 'sample_graphs' in cwd)

getarxivdatav2 needs to be run first. By default, it looks for a directory labeled '1506', but it accepts the name of a folder in the same directory.

The script generates a .txt file with the same name  as the folder (e.g. metadata of tex files in the folder named sample will be stored in 'sample.txt').

parsetex takes the same single argument as getarxivdatav2 (the folder name, with no forward slashes), and will look for the corresponding metadata file generated by getarxivdatav2, before generating counts & histograms.

--PARSETEX.py--

This can run after one of the getarxivdata scripts has run.

It reads data in from arxivdata.txt, and stores the file names and corresponding general categories in a dictionary, for lookup when iterating through while getting tokens.

Runtime on my system clocked in at 17.37 seconds, to iterate over all

After running time python parsetex.py (originally):
    real	0m17.370s
    user	0m17.796s
    sys	0m4.856s

After the addition of multiprocessing:
    real	0m4.826s
    user	0m26.252s
    sys	0m3.460s


Old/deprecated versions:
    parsetex_pre_multithreading_v1 is the original looping parsetex file (taking ~17 seconds to run), with only multiprocessing on importing the metadata generated by getarxivdatav2.py

    parsetex_multithreading_v2 features multithreaded counting of dictionaries. Note that this is thread-safe, as each core counts to its own dictionary, which is then merged with the overall category dictionaries serially (took ~7 seconds to run)


--GETARXIVDATAV2.py--

Multithreaded version of my code from gad_original.txt.
I'm reluctant to try to write other ways/make more requests per second, on account of not knowing whether or not the arXiv server has DDOS detection or would otherwise not take kindly to being flooded with queries.

This only really needs to be run once, anyway, and it takes about an hour per ~8300 files.

By far, the longest part of running getarxivdatav2.py is the networking/query portion.

CPU usage never exceeded 20% on any cores, even while I was doing other work.
There was an occasional pause as it sent requests to the arxiv server, particularly with the multithreaded version.

getarxivdatav2 had only a ~2x speedup despite 8 requests to the server being made at any given moment. Further multithreading wouldn'tve made much of a difference, as each core waited to hear back from the server before going on to the next one.
